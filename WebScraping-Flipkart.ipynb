{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flipkart Mobile Details Scraping\n",
    "\n",
    "- Enter number of page to scrap and Brand name\n",
    "- URL is dynamic (Only for mobiles of some brand availabe in flipkart mibile section)\n",
    "\n",
    "- It will auto scrap the data then clean and save it for you in .CSV/.xlsx format.\n",
    "\n",
    "[Shorten Link Article](https://towardsdatascience.com/best-apis-for-url-shortening-using-python-2db09d1f86f0)\n",
    "\n",
    ">  **Brand Name**\n",
    "- Mi, Realme, Samsung, Infinix, Apple, OPPO, Vivo\n",
    "\n",
    "Author @ Krishn Kant Raj\n",
    "\n",
    "Dated: 14/05/2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bitly_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import important libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bitly_api\n",
    "\n",
    "# Bitly Access Token\n",
    "BITLY_ACCESS_TOKEN = '749d6a714f1342aaced60e021f4a05ced8c7841d'\n",
    "access = bitly_api.Connection(access_token = BITLY_ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create empty lists\n",
    "Name = []\n",
    "Price = []\n",
    "Rating = []\n",
    "Review = []\n",
    "Links = []\n",
    "\n",
    "# Take input of brand name\n",
    "brand = input('Enter Mobile Brand Name : ')\n",
    "# Make it in title format\n",
    "brand = brand.title().split()\n",
    "# Take the brand name only\n",
    "brand = brand[0]\n",
    "    \n",
    "try:\n",
    "#     Dynamic URL\n",
    "    flipkart_url = f'https://www.flipkart.com/mobiles/pr?sid=tyy%2C4io&otracker=categorytree&p%5B%5D=facets.brand%255B%255D%3D{brand}&otracker=nmenu_sub_Electronics_0_{brand}'.format(brand)\n",
    "except InvalidSchema as er:\n",
    "    print(er)\n",
    "    print('Invalid Input')\n",
    "    \n",
    "# Show URL    \n",
    "print(flipkart_url)\n",
    "\n",
    "# Take how many page to scrap for given mobile brand \n",
    "pg_num = int(input('Enter Numbers of Page to scrap: '))\n",
    "\n",
    "for i in range(1,pg_num+1):\n",
    "    url = flipkart_url+\"&page=\"+str(i)\n",
    "    try:\n",
    "        req = requests.get(url)\n",
    "    except ConnectionError as er:\n",
    "        print(er)\n",
    "        print('Please check your connection!')\n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "    name = soup.find_all('div',{\"class\":\"_4rR01T\"})\n",
    "    price = soup.find_all('div',{\"class\":\"_30jeq3 _1_WHN1\"})\n",
    "    ratings_reviews = soup.find_all('span',{\"class\":\"_2_R_DZ\"})\n",
    "    links = soup.find_all('a',{'class':'_1fQZEK'})\n",
    "    print(\"[INFO] Phones in Page \"+str(i)+' is '+str(len(name)))\n",
    "    \n",
    "    if len(name)==0:\n",
    "        print('Invalid URL! Please check the enered URL.')\n",
    "        exit(0)\n",
    "\n",
    "    for i in name:\n",
    "        Name.append(i.text)\n",
    "    for i in price:\n",
    "        Price.append(i.text)\n",
    "    for i in ratings_reviews:\n",
    "        Rating.append(i.span.span.text)\n",
    "        Review.append(i.span.text)\n",
    "    print('[INFO] Shortening link...')\n",
    "    for i in links:\n",
    "        linktext = 'https://www.flipkart.com'+i.get('href')\n",
    "        short_url = access.shorten(linktext) \n",
    "        Links.append(short_url['url'])\n",
    "    if len(name)<24:\n",
    "        break\n",
    "    \n",
    "print(\"[INFO] Raw data scraped.\")\n",
    "print('\\n***ACKNOWLEDGEMENT***')        \n",
    "print('Name Count=',len(Name))\n",
    "print('Price Count=',len(Price))\n",
    "print('Rating Count=',len(Rating))\n",
    "print('Review Count=',len(Review))\n",
    "print('Links Count=',len(Links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (len(Name)==len(Price) and len(Name)==len(Rating) and len(Name)==len(Review) and len(Name)==len(Links)):\n",
    "    print('Woo hoo! No missing Data found')\n",
    "if len(Price)<len(Name):\n",
    "    for i in range(0,(len(Name)-len(Price))):\n",
    "        Price.append('None')\n",
    "if len(Rating)<len(Name):\n",
    "    for i in range(0,(len(Name)-len(Rating))):\n",
    "        Rating.append('None')\n",
    "if len(Review)<len(Name):\n",
    "    for i in range(0,(len(Name)-len(Review))):\n",
    "        Review.append('None')\n",
    "if len(Links)<len(Name):\n",
    "    for i in range(0,(len(Name)-len(Links))):\n",
    "        Links.append('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  See missing data filled with None or not\n",
    "#  Get the length of all lists\n",
    "print(len(Name))\n",
    "print(len(Price))\n",
    "print(len(Review))\n",
    "print(len(Rating))\n",
    "print(len(Links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove noise form price and clean it\n",
    "clean_price = []\n",
    "for i in range(len(Price)):\n",
    "    price = Price[i][1:].replace(',','')\n",
    "    clean_price.append(price)\n",
    "clean_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Reviews counts\n",
    "# Clean the data\n",
    "clean_review = []\n",
    "for i in range(len(Review)):\n",
    "    if Review[i]=='None':\n",
    "        clean_review.append('None')\n",
    "    else:\n",
    "        Review_str = Review[i].split('\\xa0&\\xa0',1)[1]\n",
    "        Review_str = Review_str.replace(' Reviews','').replace(',','')\n",
    "        clean_review.append(Review_str)\n",
    "clean_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Rating counts\n",
    "# clean the data\n",
    "clean_rating = []\n",
    "for i in range(len(Rating)):\n",
    "    if Rating[i]=='None':\n",
    "        clean_rating.append('None')\n",
    "    else:\n",
    "        Rating_str = Rating[i][:-1]\n",
    "        Rating_str = Rating_str.replace(' Ratings','').replace(',','')\n",
    "        clean_rating.append(Rating_str)\n",
    "clean_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary and convert to dataframe\n",
    "data = {\n",
    "    'Product_Name':Name,\n",
    "    'Price':clean_price,\n",
    "    'Rating':clean_rating,\n",
    "    'Review':clean_review,\n",
    "    'Link':Links\n",
    "}\n",
    "data = pd.DataFrame(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the data\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your string constraint is not just one string you can drop those corresponding rows with:\n",
    "# df = df[~df['your column'].isin(['list of strings'])]\n",
    "\n",
    "# Remove all rows having 'None' string\n",
    "data = data[~data.Rating.str.contains(\"None\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the info\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all object to numeric form\n",
    "data[['Price','Rating','Review']] = data[['Price','Rating','Review']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get updated info\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Last five rows\n",
    "data.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First five rows\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Data\n",
    "# Randomly selected\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save file with brand name\n",
    "filename = brand+'.xlsx'   \n",
    "try:    \n",
    "    writer = pd.ExcelWriter(filename, engine='xlsxwriter', options={'strings_to_urls': True})\n",
    "    data.to_excel(writer,index=False)\n",
    "    print(\"['INFO'] File saved successfully!\")\n",
    "except PermissionError as pr:\n",
    "    print(pr)\n",
    "    print('Close the pre opended file with same file name')\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
